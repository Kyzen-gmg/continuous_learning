Building a Security Assessment and Testing Program

# Security tests 
Security Testing Security tests verify that a control is functioning properly. These tests include automated scans, tool-assisted penetration tests, and manual attempts to undermine security. Security testing should take place on a regular schedule, with attention paid to each of the key security controls protecting an organization. When scheduling security controls for review, information security managers should consider the following factors: 
- Availability of security testing resources 
- Criticality of the systems and applications protected by the tested controls 
- Sensitivity of information contained on tested systems and applications 
- Likelihood of a technical failure of the mechanism implementing the control 
- Likelihood of a misconfiguration of the control that would jeopardize security 
- Risk that the system will come under attack 
- Rate of change of the control configuration 
- Other changes in the technical environment that may affect the control performance 
- Difficulty and time required to perform a control test 
- Impact of the test on normal business operations 

After assessing each of these factors, security teams design and validate a comprehensive assessment and testing strategy. This strategy may include frequent automated tests supplemented by infrequent manual tests. For example, a credit card processing system may undergo automated vulnerability scanning on a nightly basis with immediate alerts to administrators when the scan detects a new vulnerability. The automated scan requires no work from administrators once it is configured, so it is easy to run quite frequently. The security team may wish to complement those automated scans with a manual penetration test performed by an external consultant for a significant fee. Those tests may occur on an annual basis to minimize costs and disruption to the business.

# Security assessments 
Security assessments are comprehensive reviews of the security of a system, application, or other tested environment. During a security assessment, a trained information security professional performs a risk assessment that identifies vulnerabilities in the tested environment that may allow a compromise and makes recommendations for remediation, as needed. Security assessments normally include the use of security testing tools but go beyond automated scanning and manual penetration tests. They also include a thoughtful review of the threat environment, current and future risks, and the value of the targeted environment.

# Security audits
Security audits use many of the same techniques followed during security assessments but must be performed by independent auditors. An organization's security staff may routinely perform security tests and assessments, but this is not the case for audits. Assessment and testing results are meant for internal use only and are designed to evaluate controls with an eye toward finding potential improvements. Audits, on the other hand, are evaluations performed with the purpose of demonstrating the effectiveness of controls to a third party. The staff who design, implement, and monitor controls for an organization have an inherent conflict of interest when evaluating the effectiveness of those controls.

SSAE 18 and ISAE 3402 engagements are commonly referred to as service organization controls (SOC) audits, and they come in three forms: 
- SOC 1 Engagements Assess the organization's controls that might impact the accuracy of financial reporting. 
- SOC 2 Engagements Assess the organization's controls that affect the security (confidentiality, integrity, and availability) and privacy of information stored in a system. SOC 2 audit results are confidential and are normally only shared outside the organization under an NDA. 
- SOC 3 Engagements Assess the organization's controls that affect the security (confidentiality, integrity, and availability) and privacy of information stored in a system. 
However, SOC 3 audit results are intended for public disclosure. 

In addition to the three categories of SOC assessment, there are two different types of SOC report. Both reports begin with providing a description by management of the controls put in place. They differ in the scope of the opinion provided by the auditor: 
- Type I Reports These reports provide the auditor's opinion on the description provided by management and the suitability of the design of the controls. Type I reports also cover only a specific point in time, rather than an extended period. You can think of the Type I report as more of a documentation review where the auditor is checking things out on paper and making sure that the controls described by management are reasonable and appropriate. 
- Type II Reports These reports go further and also provide the auditor's opinion on the operating effectiveness of the controls. That is, the auditor actually confirms that the controls are functioning properly. The Type II report also covers an extended period of time: at least six months of operation. You can think of the Type II report as more like a traditional audit. The auditors are not just checking the paperwork; they are also going in and verifying that the controls function properly. Type II reports are considered much more reliable than Type I reports because they include independent testing of controls. Type I reports simply take the service organization at their word that the controls are implemented as described.

# Vulnerability Management Workflow 
Organizations that adopt a vulnerability management system should also develop a workflow approach to managing vulnerabilities. The basic steps in this workflow should include the following: 
1. Detection: The initial identification of a vulnerability normally takes place as the result of a vulnerability scan. 
2. Validation: Once a scanner detects a vulnerability, administrators should confirm the vulnerability to determine that it is not a false positive report. 
3. Remediation: Validated vulnerabilities should then be remediated. This may include applying a vendor-supplied security patch, modifying a device configuration, implementing a workaround to avoid the vulnerability, or installing a web application firewall or other control that prevents the exploitation of the vulnerability.

# Test Coverage Analysis
test coverage = # of use cases tested / total number of use cases

The test coverage analysis formula may be adapted to use many different criteria. Here are five common criteria: 
- Branch coverage: Has every if statement been executed under all if and else conditions? 
- Condition coverage: - Has every logical test in the code been executed under all sets of inputs? 
- Function coverage: Has every function in the code been called and returned results? 
- Loop coverage: Has every loop in the code been executed under conditions that cause code execution multiple times, only once, and not at all? 
- Statement coverage: Has every line of code been executed during the test?

# Website Monitoring 
Security professionals also often become involved in the ongoing monitoring of websites for performance management, troubleshooting, and the identification of potential security issues. This type of monitoring comes in two different forms: 
1. Passive monitoring analyzes actual network traffic sent to a website by capturing it as it travels over the network or reaches the server. This provides real-world monitoring data that gives administrators insight into what is actually happening on a network. Real user monitoring (RUM) is a variant of passive monitoring where the monitoring tool reassembles the activity of individual users to track their interaction with a website. 
2. Synthetic monitoring (or active monitoring) performs artificial transactions against a website to assess performance. This may be as simple as requesting a page from the site to determine the response time, or it may execute a complex script designed to identify the results of a transaction.

# Key Performance and Risk Indicators 
Security managers should also monitor key performance and risk indicators on an ongoing basis. The exact metrics they monitor will vary from organization to organization but may include the following: 
- Number of open vulnerabilities Time to resolve vulnerabilities 
- Vulnerability/defect recurrence Number of compromised accounts 
- Number of software flaws detected in preproduction scanning 
- Repeat audit findings 
- User attempts to visit known malicious sites

# Interviews
It is important to include personnel who are not only experienced in the complexities of systems and processes, but also have the ability to probe for areas of risk. A checklist is a good guideline, but is only the starting point in the process. With an experienced interviewer, the process can be as educational for the interviewee as it is for identifying risks.   
Organizational executives have limited time, and it is often difficult to get on their calendars. There are three key steps to ease this part of the process: 
1. Request that the executive sponsor directly address the interviewees by announcing the purpose of the risk assessment and its importance to the organization. 
2. Within 48 hours of that communication, have the sponsorâ€™s office schedule the initial interview. 
3. Send a tailored checklist to the executive prior to the interview and ask him/her to review it. This last step is to prepare him/her for the subject areas of the risk assessment, so that any apprehensions or reservations are allayed as he/ she understands the boundaries of the interview.   
It is important not to underestimate the value of an experienced facilitator, particularly for the higher-level interviews and the process of determining the ranking of risk likelihood. The use of experienced external resources should be considered to bring even more objectivity to the assessment.